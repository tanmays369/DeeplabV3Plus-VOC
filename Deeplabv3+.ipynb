{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deeplabv3+.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d3ljKOgRE4b"
      },
      "source": [
        "import os\r\n",
        "import torch\r\n",
        "from torch.utils.data import DataLoader, Dataset\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.transforms.functional as TrF\r\n",
        "from torch.hub import load_state_dict_from_url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIy73-KZRBQG"
      },
      "source": [
        "root= \"content/\"\r\n",
        "imgs= root+\"JPEG/\"\r\n",
        "masks= root+\"SegmentationClass/\"\r\n",
        "f_names= \"ImageSets/Segmentation/\"\r\n",
        "\r\n",
        "ids= {\r\n",
        "    'train': \"ImageSets/Segmentation/train.txt\",\r\n",
        "    'val': \"ImageSets/Segmentation/val.txt\"\r\n",
        "    }\r\n",
        "\r\n",
        "\r\n",
        "class VOCPascal(Dataset):\r\n",
        "    def __init__(self, ids, path_to_imgs, path_to_masks, mode= 'train'):\r\n",
        "        data= ids[mode]\r\n",
        "        with open(data, 'r') as f:\r\n",
        "            fnames= f.read().split()\r\n",
        "        self.imgs= [os.path.join(path_to_imgs+img+'.jpg') for img in fnames]\r\n",
        "        self.masks= [os.path.join(path_to_masks+mask+'.png') for mask in fnames]\r\n",
        "        self.mode= mode\r\n",
        "\r\n",
        "    def __getitem__(self, ix):\r\n",
        "        img, mask= self.imgs[ix], self.masks[ix]\r\n",
        "        img, mask= Image.open(img), Image.open(mask)\r\n",
        "        img, mask= self._transform(img, mask)\r\n",
        "        return img, mask\r\n",
        "    \r\n",
        "    def _transform(self, img, mask):\r\n",
        "        means, stdev= [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\r\n",
        "        if self.mode== 'train' and random.random()> 0.5:\r\n",
        "            img, mask= TrF.hflip(img), TrF.hflip(mask)\r\n",
        "        img, mask= TrF.to_tensor(img), np.array(mask, np.int64)\r\n",
        "        mask[mask== 255]= -1\r\n",
        "        img= TrF.normalize(img, mean= means, std= stdev)\r\n",
        "        mask= torch.from_numpy(mask)\r\n",
        "        return img, mask\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH9b_9mFCYW5"
      },
      "source": [
        "class Block(nn.Module):\r\n",
        "    def __init__(self, in_channels, out_channels, stride=1, skip=None):\r\n",
        "        super(Bottleneck, self).__init__()\r\n",
        "        self.conv1= nn.Conv2d(in_channels, out_channels, 1, bias= False)\r\n",
        "        self.bn1= nn.BatchNorm2d(out_channels)\r\n",
        "        self.conv2= nn.Conv2d(out_channels, out_channels, 3, stride, bias= False)\r\n",
        "        self.bn2= nn.BatchNorm2d(out_channels)\r\n",
        "        self.conv3= nn.Conv2d(out_channels, 4*out_channels, 1, bias= False)\r\n",
        "        self.bn3= nn.BatchNorm2d(4*out_channels)\r\n",
        "        self.skip= skip\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        inputs= x\r\n",
        "        x= self.conv1(x)\r\n",
        "        x= F.relu(self.bn1(x))\r\n",
        "        x= self.conv2(x)\r\n",
        "        x= F.relu(self.bn2(x))\r\n",
        "        x= self.bn3(self.conv3(x))\r\n",
        "        if self.skip is not None:\r\n",
        "            inputs= self.skip(x)\r\n",
        "        x+= inputs\r\n",
        "        return F.relu(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tz1W0D3mDk5o"
      },
      "source": [
        "class ResNet(nn.Module):\r\n",
        "    def __init__(self, block, layers, n_classes=1000)\r\n",
        "        super(ResNet, self).__init__()\r\n",
        "        layers= [3, 4, 23, 3]\r\n",
        "        self.in_channels= 64\r\n",
        "        channels= 64\r\n",
        "        self.conv1= nn.Conv2d(3, channels, 3, 2, 1, bias=False)\r\n",
        "        self.bn1= nn.BatchNorm2d(channels)\r\n",
        "        self.conv2= nn.Conv2d(channels, channels, 3, 1, 1, bias=False)\r\n",
        "        self.bn2= nn.BatchNorm2d(channels)\r\n",
        "        self.conv3= nn.Conv2d(channels, 2*channels, 3, 1, 1, bias=False)\r\n",
        "        self.bn3= nn.BatchNorm2d(channels)\r\n",
        "        self.layer1= self.make_layers(block, channels, 3)\r\n",
        "        self.layer2= self.make_layers(block, 2*channels, 4, 2)\r\n",
        "        self.layer3= self.make_layers(block, 4*channels, 23, 2)\r\n",
        "        self.layer4= self.make_layers(block, 8*channels, 3, 2)\r\n",
        "        self.fc1= nn.Linear(2048, num_classes)\r\n",
        "    def make_layers(self, block, channels, num_blocks, stride=1):\r\n",
        "        skip= None\r\n",
        "        if self.in_channels != 4*channels or stride != 1:\r\n",
        "            skip = nn.Sequential(\r\n",
        "                nn.Conv2d(self.in_channels, 4*channels, 1, stride, bias= False),\r\n",
        "                nn.BatchNorm2d(4*channels),\r\n",
        "            )\r\n",
        "        layers = []\r\n",
        "        b= block(self.in_channels, channels, stride, skip)\r\n",
        "        layers.append(b)\r\n",
        "        self.in_channels= 4*channels\r\n",
        "        for _ in range(1, num_blocks):\r\n",
        "            b= block(self.in_channels, channels)\r\n",
        "            layers.append(b)\r\n",
        "        return nn.Sequential(*layers)\r\n",
        "    def forward(self, x):\r\n",
        "        x= self.conv1(x)\r\n",
        "        x= F.relu(self.bn1(x))\r\n",
        "        x= F.max_pool2d(x, 3, 2, 1)\r\n",
        "        x= self.layer1(x)\r\n",
        "        x = self.layer2(x)\r\n",
        "        x = self.layer3(x)\r\n",
        "        x = self.layer4(x)\r\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\r\n",
        "        x= torch.flatten(x, 1)\r\n",
        "        return self.fc1(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmAHAkUqDtTH"
      },
      "source": [
        "model= ResNet(Block, n_classes) #n_classses\r\n",
        "url= \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\"\r\n",
        "state= load_state_dict_from_url(url)\r\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcx1Ysp2DuQa"
      },
      "source": [
        "class ConvBlock(nn.Module):\r\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, \r\n",
        "                stride= 1, padding= 0, dilation= 1, bias= False,\r\n",
        "                use_norm= False, use_act= False):\r\n",
        "        super(ConvBlock, self).__init__()\r\n",
        "        self.use_norm= use_norm\r\n",
        "        self.use_act= use_act\r\n",
        "        self.conv= nn.Conv2d(in_channels, out_channels, kernel_size,\r\n",
        "                        stride, padding, dilation, bias= bias)\r\n",
        "        self.bn= nn.BatchNorm2d(out_channels)\r\n",
        "    def forward(self, x):\r\n",
        "        x= self.conv(x)\r\n",
        "        if self.use_norm:\r\n",
        "            x= self.bn(x)\r\n",
        "        if self.use_act:\r\n",
        "            x= F.relu(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9iqetViNrYp"
      },
      "source": [
        "class ASPP(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(ASPP, self).__init__()\r\n",
        "        in_channels, out_channels= 2048, 256\r\n",
        "        dilations= [1, 6, 12, 18]\r\n",
        "        self.aspp1= ConvBlock(in_channels, out_channels, 1, 1, 0, dilations[0], use_norm= True, use_act= True)\r\n",
        "        self.aspp2= ConvBlock(in_channels, out_channels, 3, 1, dilations[1], dilations[1], use_norm= True, use_act= True)\r\n",
        "        self.aspp3= ConvBlock(in_channels, out_channels, 3, 1, dilations[2], dilations[2], use_norm= True, use_act= True)\r\n",
        "        self.aspp4= ConvBlock(in_channels, out_channels, 3, 1, dilations[3], dilations[3], use_norm= True, use_act= True)\r\n",
        "        self.conv= ConvBlock(in_channels, out_channels, 1, use_norm= True, use_act= True)\r\n",
        "        self.out_conv= ConvBlock(4*out_channels, out_channels, 1, use_norm= True, use_act= True)\r\n",
        "    def forward(self, x):\r\n",
        "        x1= self.aspp1(x)\r\n",
        "        x2= self.aspp2(x)\r\n",
        "        x3= self.aspp3(x)\r\n",
        "        x4= self.aspp4(x)\r\n",
        "        x5= self.conv(F.adaptive_avg_pool2d(x, (1, 1)))\r\n",
        "        x= torch.cat([x1, x2, x3, x4, x5], 1)\r\n",
        "        x= self.out_conv(x)\r\n",
        "        x= F.dropout(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbkfAdTONxSc"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, n_classes):\r\n",
        "        super(Decoder, self).__init__()\r\n",
        "        channels= 256\r\n",
        "        self.conv1= ConvBlock(channels, 48, 1, use_norm= True, use_act= True)\r\n",
        "        self.conv2= ConvBlock(304, channels, 3, 1, 1, use_norm= True, use_act= True)\r\n",
        "        self.conv3= ConvBlock(channels, channels, 3, 1, 1, use_norm= True, use_act= True)\r\n",
        "        self.conv4= ConvBlock(channels, n_classes, 1)\r\n",
        "    def forward(self, x, x1):\r\n",
        "        x1= self.conv1(x1)\r\n",
        "        x= F.interpolate(x, x1.size()[2:], mode= \"bilinear\", align_corners= True)\r\n",
        "        x= torch.cat([x, x1], 1)\r\n",
        "        x= F.dropout(self.conv2(x))\r\n",
        "        x= F.dropout(self.conv3(x), 0.1)\r\n",
        "        x= self.conv4(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ungEBn1LNyVK"
      },
      "source": [
        "class DeeplabV3Plus(nn.Module):\r\n",
        "    def __init__(self, n_classes):\r\n",
        "        super(DeeplabV3Plus, self).__init__()\r\n",
        "        self.resnet= \r\n",
        "        self.aspp= ASPP()\r\n",
        "        self.decode= Decoder(n_classes)\r\n",
        "    def forward(self, x):\r\n",
        "        x1, x2= self.resnet(x)\r\n",
        "        x1= self.aspp(x)\r\n",
        "        x1= self.decoder(x1, x2)\r\n",
        "        x= F.interpolate(x1, x.size()[2:], mode= \"bilinear\", align_corners= True)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz1bWfoFPaOZ"
      },
      "source": [
        "for epoch in range(0, num_epochs):\r\n",
        "    model.train()\r\n",
        "    running_loss= 0\r\n",
        "    for batch_ix, (imgs, masks) in trainloader:\r\n",
        "        imgs, masks= imgs.cuda(), masks.cuda()\r\n",
        "        optimizer.zero_grad()\r\n",
        "        outs= model(imgs)\r\n",
        "        loss= loss_fn(outs, masks)\r\n",
        "        running_loss+= loss.item()\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "    print(f\"Train loss: {running_loss/len(trainloader)}\")\r\n",
        "    valid_loss= 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for batch_ix, (imgs, masks) in validloader:\r\n",
        "            imgs, masks= imgs.cuda(), masks.cuda()\r\n",
        "            outs= model(imgs)\r\n",
        "            loss= loss_fn(outs, masks)\r\n",
        "            valid_loss+= loss.item()\r\n",
        "    print(f\"Validation loss: {valid_loss/len(validloader)}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}